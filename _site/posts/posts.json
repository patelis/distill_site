[
  {
    "path": "posts/2021-04-11-classification-modeling-workflow-using-tidymodels/",
    "title": "Classification modelling workflow using tidymodels",
    "description": "In this post we demonstrate a complete modelling workflow to solve a classification problem using the tidymodels ecosystem of packages. We'll pre-process the data, set up different models and tune their different hyperparameters. In the end we will see which model performs best, based on its out-of-sample accuracy and even try our hand at creating an ensemble model.",
    "author": [
      {
        "name": "Konstantinos Patelis",
        "url": "https://kpatelis.com/"
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "classification",
      "tidymodels",
      "stacks"
    ],
    "contents": "\r\nI was recently working through the final assignment in the Practical Machine Learning Coursera course (part of the JHU Data Science Specialization), which entailed creating a model to predict the way people perform a weight-lifting exercise using data from accelerometers on the belt, forearm, arm, and dumbell of each participant. I thought this was a good opportunity to practice using the tidymodels family of packages to tackle this classification problem. So, in this post we will go through the series of steps to create our predictive model. We will cover defining the data pre-processing, specifying the model(s) to fit and using cross-validation to tune model hyperparameters. Additionally, we’ll have a look at one of the recent additions to the tidymodels packages, stacks to create an ensemble model out of our base models. We’ll see that most of our models perform almost equally well and an ensemble model is not required for achieving improved accuracy, and is presented mostly because this was a good opportunity to try it out 😄.\r\nData\r\nWe’ll use the Weight Lifting Exercises data set (Velloso et al. 2013), provided as part of the Human Activity Recognition project. The data available to us is split in two parts, one is the training set, which contains the classe variable which we want to train our model to predict, and the quiz set, that contains 20 observations for which we needed to predict classe as part of the course assignment. For this blog post we’ll focus on the first data set, which we will split in two parts, one used for actually training the model and one to assess its accuracy.\r\n\r\n\r\n## Libraries\r\n\r\n# General data wrangling\r\nlibrary(tidyverse)\r\nlibrary(skimr)\r\n\r\n# Modeling packages\r\nlibrary(tidymodels)\r\nlibrary(stacks)\r\n\r\n# Visualization\r\nlibrary(corrr)\r\nlibrary(plotly)\r\n\r\n# Parallelization\r\nlibrary(doParallel)\r\n\r\n# EDA - will not be showing the outputs from using these packages, but very useful for exploration\r\n# library(DataExplorer)\r\n# library(explore)\r\n\r\ntheme_set(theme_bw())\r\n\r\n\r\n\r\n\r\n\r\n## Data\r\n\r\ninitial_data <- read_data(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\") %>% select(-1) # read_data is a wrapper around read_csv\r\n\r\n\r\n\r\nEDA\r\nLet’s split the initial data a training set and an test set (80/20 split). For the exploratory analysis and subsequent modeling, hyperparameter tuning and model evaluation I will use the training data set. Then the model will be used on the test data to predict out-of-sample accuracy.\r\n\r\n\r\nset.seed(1992)\r\n                            \r\nsplit <- initial_split(initial_data, prop = .8, strata = classe)\r\n\r\ntrain <-  training(split)\r\ntest <- testing(split)\r\n\r\nskim(train)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ntrain\r\nNumber of rows\r\n15700\r\nNumber of columns\r\n159\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n105\r\nnumeric\r\n53\r\nPOSIXct\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nuser_name\r\n0\r\n1.00\r\n5\r\n8\r\n0\r\n6\r\n0\r\nraw_timestamp_part_1\r\n0\r\n1.00\r\n10\r\n10\r\n0\r\n837\r\n0\r\nraw_timestamp_part_2\r\n0\r\n1.00\r\n3\r\n6\r\n0\r\n13769\r\n0\r\nnew_window\r\n0\r\n1.00\r\n2\r\n3\r\n0\r\n2\r\n0\r\nkurtosis_roll_belt\r\n15377\r\n0.02\r\n7\r\n9\r\n0\r\n315\r\n0\r\nkurtosis_picth_belt\r\n15377\r\n0.02\r\n7\r\n9\r\n0\r\n258\r\n0\r\nkurtosis_yaw_belt\r\n15377\r\n0.02\r\n7\r\n7\r\n0\r\n1\r\n0\r\nskewness_roll_belt\r\n15377\r\n0.02\r\n7\r\n9\r\n0\r\n314\r\n0\r\nskewness_roll_belt.1\r\n15377\r\n0.02\r\n7\r\n9\r\n0\r\n271\r\n0\r\nskewness_yaw_belt\r\n15377\r\n0.02\r\n7\r\n7\r\n0\r\n1\r\n0\r\nmax_roll_belt\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n165\r\n0\r\nmax_picth_belt\r\n15377\r\n0.02\r\n1\r\n2\r\n0\r\n22\r\n0\r\nmax_yaw_belt\r\n15377\r\n0.02\r\n3\r\n7\r\n0\r\n62\r\n0\r\nmin_roll_belt\r\n15377\r\n0.02\r\n2\r\n5\r\n0\r\n161\r\n0\r\nmin_pitch_belt\r\n15377\r\n0.02\r\n1\r\n2\r\n0\r\n16\r\n0\r\nmin_yaw_belt\r\n15377\r\n0.02\r\n3\r\n7\r\n0\r\n62\r\n0\r\namplitude_roll_belt\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n124\r\n0\r\namplitude_pitch_belt\r\n15377\r\n0.02\r\n1\r\n2\r\n0\r\n13\r\n0\r\namplitude_yaw_belt\r\n15377\r\n0.02\r\n4\r\n7\r\n0\r\n3\r\n0\r\nvar_total_accel_belt\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n60\r\n0\r\navg_roll_belt\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n165\r\n0\r\nstddev_roll_belt\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n62\r\n0\r\nvar_roll_belt\r\n15377\r\n0.02\r\n1\r\n7\r\n0\r\n81\r\n0\r\navg_pitch_belt\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n191\r\n0\r\nstddev_pitch_belt\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n41\r\n0\r\nvar_pitch_belt\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n53\r\n0\r\navg_yaw_belt\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n207\r\n0\r\nstddev_yaw_belt\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n52\r\n0\r\nvar_yaw_belt\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n122\r\n0\r\nvar_accel_arm\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n313\r\n0\r\navg_roll_arm\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n261\r\n0\r\nstddev_roll_arm\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n261\r\n0\r\nvar_roll_arm\r\n15377\r\n0.02\r\n1\r\n10\r\n0\r\n261\r\n0\r\navg_pitch_arm\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n261\r\n0\r\nstddev_pitch_arm\r\n15377\r\n0.02\r\n1\r\n7\r\n0\r\n261\r\n0\r\nvar_pitch_arm\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n261\r\n0\r\navg_yaw_arm\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n261\r\n0\r\nstddev_yaw_arm\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n258\r\n0\r\nvar_yaw_arm\r\n15377\r\n0.02\r\n1\r\n10\r\n0\r\n258\r\n0\r\nkurtosis_roll_arm\r\n15377\r\n0.02\r\n7\r\n8\r\n0\r\n260\r\n0\r\nkurtosis_picth_arm\r\n15377\r\n0.02\r\n7\r\n8\r\n0\r\n258\r\n0\r\nkurtosis_yaw_arm\r\n15377\r\n0.02\r\n7\r\n8\r\n0\r\n312\r\n0\r\nskewness_roll_arm\r\n15377\r\n0.02\r\n7\r\n8\r\n0\r\n261\r\n0\r\nskewness_pitch_arm\r\n15377\r\n0.02\r\n7\r\n8\r\n0\r\n258\r\n0\r\nskewness_yaw_arm\r\n15377\r\n0.02\r\n7\r\n8\r\n0\r\n312\r\n0\r\nmax_roll_arm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n232\r\n0\r\nmax_picth_arm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n214\r\n0\r\nmax_yaw_arm\r\n15377\r\n0.02\r\n1\r\n2\r\n0\r\n49\r\n0\r\nmin_roll_arm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n228\r\n0\r\nmin_pitch_arm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n230\r\n0\r\nmin_yaw_arm\r\n15377\r\n0.02\r\n1\r\n2\r\n0\r\n37\r\n0\r\namplitude_roll_arm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n246\r\n0\r\namplitude_pitch_arm\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n236\r\n0\r\namplitude_yaw_arm\r\n15377\r\n0.02\r\n1\r\n2\r\n0\r\n51\r\n0\r\nkurtosis_roll_dumbbell\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n318\r\n0\r\nkurtosis_picth_dumbbell\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n320\r\n0\r\nkurtosis_yaw_dumbbell\r\n15377\r\n0.02\r\n7\r\n7\r\n0\r\n1\r\n0\r\nskewness_roll_dumbbell\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n320\r\n0\r\nskewness_pitch_dumbbell\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n319\r\n0\r\nskewness_yaw_dumbbell\r\n15377\r\n0.02\r\n7\r\n7\r\n0\r\n1\r\n0\r\nmax_roll_dumbbell\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n282\r\n0\r\nmax_picth_dumbbell\r\n15377\r\n0.02\r\n2\r\n6\r\n0\r\n285\r\n0\r\nmax_yaw_dumbbell\r\n15377\r\n0.02\r\n3\r\n7\r\n0\r\n64\r\n0\r\nmin_roll_dumbbell\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n277\r\n0\r\nmin_pitch_dumbbell\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n294\r\n0\r\nmin_yaw_dumbbell\r\n15377\r\n0.02\r\n3\r\n7\r\n0\r\n64\r\n0\r\namplitude_roll_dumbbell\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n313\r\n0\r\namplitude_pitch_dumbbell\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n310\r\n0\r\namplitude_yaw_dumbbell\r\n15377\r\n0.02\r\n4\r\n7\r\n0\r\n2\r\n0\r\nvar_accel_dumbbell\r\n15377\r\n0.02\r\n1\r\n7\r\n0\r\n310\r\n0\r\navg_roll_dumbbell\r\n15377\r\n0.02\r\n5\r\n9\r\n0\r\n320\r\n0\r\nstddev_roll_dumbbell\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n315\r\n0\r\nvar_roll_dumbbell\r\n15377\r\n0.02\r\n1\r\n10\r\n0\r\n315\r\n0\r\navg_pitch_dumbbell\r\n15377\r\n0.02\r\n5\r\n8\r\n0\r\n320\r\n0\r\nstddev_pitch_dumbbell\r\n15377\r\n0.02\r\n1\r\n7\r\n0\r\n315\r\n0\r\nvar_pitch_dumbbell\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n315\r\n0\r\navg_yaw_dumbbell\r\n15377\r\n0.02\r\n5\r\n9\r\n0\r\n320\r\n0\r\nstddev_yaw_dumbbell\r\n15377\r\n0.02\r\n1\r\n7\r\n0\r\n315\r\n0\r\nvar_yaw_dumbbell\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n315\r\n0\r\nkurtosis_roll_forearm\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n255\r\n0\r\nkurtosis_picth_forearm\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n255\r\n0\r\nkurtosis_yaw_forearm\r\n15377\r\n0.02\r\n7\r\n7\r\n0\r\n1\r\n0\r\nskewness_roll_forearm\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n256\r\n0\r\nskewness_pitch_forearm\r\n15377\r\n0.02\r\n6\r\n7\r\n0\r\n252\r\n0\r\nskewness_yaw_forearm\r\n15377\r\n0.02\r\n7\r\n7\r\n0\r\n1\r\n0\r\nmax_roll_forearm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n224\r\n0\r\nmax_picth_forearm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n137\r\n0\r\nmax_yaw_forearm\r\n15377\r\n0.02\r\n3\r\n7\r\n0\r\n40\r\n0\r\nmin_roll_forearm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n218\r\n0\r\nmin_pitch_forearm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n149\r\n0\r\nmin_yaw_forearm\r\n15377\r\n0.02\r\n3\r\n7\r\n0\r\n40\r\n0\r\namplitude_roll_forearm\r\n15377\r\n0.02\r\n1\r\n5\r\n0\r\n234\r\n0\r\namplitude_pitch_forearm\r\n15377\r\n0.02\r\n1\r\n6\r\n0\r\n154\r\n0\r\namplitude_yaw_forearm\r\n15377\r\n0.02\r\n4\r\n7\r\n0\r\n2\r\n0\r\nvar_accel_forearm\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n319\r\n0\r\navg_roll_forearm\r\n15377\r\n0.02\r\n1\r\n10\r\n0\r\n256\r\n0\r\nstddev_roll_forearm\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n254\r\n0\r\nvar_roll_forearm\r\n15377\r\n0.02\r\n1\r\n11\r\n0\r\n254\r\n0\r\navg_pitch_forearm\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n257\r\n0\r\nstddev_pitch_forearm\r\n15377\r\n0.02\r\n1\r\n8\r\n0\r\n257\r\n0\r\nvar_pitch_forearm\r\n15377\r\n0.02\r\n1\r\n10\r\n0\r\n257\r\n0\r\navg_yaw_forearm\r\n15377\r\n0.02\r\n1\r\n10\r\n0\r\n257\r\n0\r\nstddev_yaw_forearm\r\n15377\r\n0.02\r\n1\r\n9\r\n0\r\n255\r\n0\r\nvar_yaw_forearm\r\n15377\r\n0.02\r\n1\r\n11\r\n0\r\n255\r\n0\r\nclasse\r\n0\r\n1.00\r\n1\r\n1\r\n0\r\n5\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nnum_window\r\n0\r\n1\r\n431.53\r\n248.39\r\n1.00\r\n223.00\r\n425.00\r\n647.00\r\n864.00\r\n▇▇▇▇▇\r\nroll_belt\r\n0\r\n1\r\n64.50\r\n62.76\r\n-28.90\r\n1.10\r\n113.00\r\n123.00\r\n162.00\r\n▇▁▁▅▅\r\npitch_belt\r\n0\r\n1\r\n0.39\r\n22.33\r\n-55.80\r\n1.79\r\n5.31\r\n15.20\r\n60.30\r\n▃▁▇▅▁\r\nyaw_belt\r\n0\r\n1\r\n-11.33\r\n95.00\r\n-180.00\r\n-88.30\r\n-12.95\r\n12.53\r\n179.00\r\n▁▇▅▁▃\r\ntotal_accel_belt\r\n0\r\n1\r\n11.33\r\n7.74\r\n0.00\r\n3.00\r\n17.00\r\n18.00\r\n29.00\r\n▇▁▂▆▁\r\ngyros_belt_x\r\n0\r\n1\r\n-0.01\r\n0.20\r\n-0.98\r\n-0.03\r\n0.03\r\n0.11\r\n2.02\r\n▁▇▁▁▁\r\ngyros_belt_y\r\n0\r\n1\r\n0.04\r\n0.08\r\n-0.64\r\n0.00\r\n0.02\r\n0.11\r\n0.64\r\n▁▁▇▁▁\r\ngyros_belt_z\r\n0\r\n1\r\n-0.13\r\n0.24\r\n-1.46\r\n-0.20\r\n-0.10\r\n-0.02\r\n1.62\r\n▁▂▇▁▁\r\naccel_belt_x\r\n0\r\n1\r\n-5.69\r\n29.69\r\n-120.00\r\n-21.00\r\n-15.00\r\n-5.00\r\n85.00\r\n▁▁▇▁▂\r\naccel_belt_y\r\n0\r\n1\r\n30.21\r\n28.63\r\n-69.00\r\n3.00\r\n35.00\r\n61.00\r\n164.00\r\n▁▇▇▁▁\r\naccel_belt_z\r\n0\r\n1\r\n-72.80\r\n100.47\r\n-275.00\r\n-162.00\r\n-152.00\r\n27.00\r\n105.00\r\n▁▇▁▅▃\r\nmagnet_belt_x\r\n0\r\n1\r\n55.39\r\n63.78\r\n-52.00\r\n9.00\r\n35.00\r\n59.00\r\n485.00\r\n▇▁▂▁▁\r\nmagnet_belt_y\r\n0\r\n1\r\n593.65\r\n35.70\r\n354.00\r\n581.00\r\n601.00\r\n610.00\r\n673.00\r\n▁▁▁▇▃\r\nmagnet_belt_z\r\n0\r\n1\r\n-345.46\r\n66.00\r\n-621.00\r\n-375.00\r\n-319.00\r\n-306.00\r\n293.00\r\n▁▇▁▁▁\r\nroll_arm\r\n0\r\n1\r\n17.52\r\n72.88\r\n-180.00\r\n-32.40\r\n0.00\r\n77.10\r\n180.00\r\n▁▃▇▆▂\r\npitch_arm\r\n0\r\n1\r\n-4.80\r\n30.78\r\n-88.20\r\n-26.20\r\n0.00\r\n11.10\r\n88.50\r\n▁▅▇▂▁\r\nyaw_arm\r\n0\r\n1\r\n-0.81\r\n71.30\r\n-180.00\r\n-43.12\r\n0.00\r\n45.10\r\n180.00\r\n▁▃▇▃▂\r\ntotal_accel_arm\r\n0\r\n1\r\n25.56\r\n10.55\r\n1.00\r\n17.00\r\n27.00\r\n33.00\r\n65.00\r\n▃▆▇▁▁\r\ngyros_arm_x\r\n0\r\n1\r\n0.04\r\n1.99\r\n-6.37\r\n-1.33\r\n0.08\r\n1.57\r\n4.87\r\n▁▃▇▆▂\r\ngyros_arm_y\r\n0\r\n1\r\n-0.26\r\n0.85\r\n-3.44\r\n-0.80\r\n-0.24\r\n0.16\r\n2.84\r\n▁▂▇▂▁\r\ngyros_arm_z\r\n0\r\n1\r\n0.27\r\n0.55\r\n-2.33\r\n-0.07\r\n0.23\r\n0.72\r\n3.02\r\n▁▂▇▂▁\r\naccel_arm_x\r\n0\r\n1\r\n-59.78\r\n182.57\r\n-404.00\r\n-242.00\r\n-43.00\r\n84.00\r\n437.00\r\n▇▅▇▅▁\r\naccel_arm_y\r\n0\r\n1\r\n32.22\r\n110.13\r\n-318.00\r\n-54.00\r\n14.00\r\n139.00\r\n308.00\r\n▁▃▇▇▂\r\naccel_arm_z\r\n0\r\n1\r\n-71.75\r\n134.81\r\n-629.00\r\n-144.00\r\n-47.00\r\n22.00\r\n271.00\r\n▁▁▅▇▁\r\nmagnet_arm_x\r\n0\r\n1\r\n192.26\r\n443.84\r\n-584.00\r\n-299.00\r\n292.00\r\n637.00\r\n782.00\r\n▆▃▂▃▇\r\nmagnet_arm_y\r\n0\r\n1\r\n155.90\r\n201.81\r\n-386.00\r\n-10.00\r\n200.00\r\n322.00\r\n582.00\r\n▁▅▅▇▂\r\nmagnet_arm_z\r\n0\r\n1\r\n305.14\r\n327.76\r\n-597.00\r\n128.00\r\n442.50\r\n545.00\r\n694.00\r\n▁▂▂▃▇\r\nroll_dumbbell\r\n0\r\n1\r\n23.34\r\n69.96\r\n-152.83\r\n-19.74\r\n48.01\r\n67.46\r\n153.55\r\n▂▂▃▇▂\r\npitch_dumbbell\r\n0\r\n1\r\n-10.84\r\n37.04\r\n-149.59\r\n-40.98\r\n-20.92\r\n17.21\r\n137.02\r\n▁▅▇▃▁\r\nyaw_dumbbell\r\n0\r\n1\r\n1.77\r\n82.63\r\n-150.87\r\n-77.76\r\n-2.45\r\n79.65\r\n154.95\r\n▃▇▅▅▆\r\ntotal_accel_dumbbell\r\n0\r\n1\r\n13.68\r\n10.22\r\n0.00\r\n4.00\r\n10.00\r\n19.00\r\n58.00\r\n▇▅▃▁▁\r\ngyros_dumbbell_x\r\n0\r\n1\r\n0.16\r\n1.68\r\n-204.00\r\n-0.03\r\n0.13\r\n0.35\r\n2.22\r\n▁▁▁▁▇\r\ngyros_dumbbell_y\r\n0\r\n1\r\n0.05\r\n0.64\r\n-2.10\r\n-0.14\r\n0.05\r\n0.21\r\n52.00\r\n▇▁▁▁▁\r\ngyros_dumbbell_z\r\n0\r\n1\r\n-0.12\r\n2.55\r\n-2.38\r\n-0.31\r\n-0.13\r\n0.03\r\n317.00\r\n▇▁▁▁▁\r\naccel_dumbbell_x\r\n0\r\n1\r\n-28.70\r\n67.30\r\n-419.00\r\n-50.00\r\n-8.00\r\n11.00\r\n235.00\r\n▁▁▆▇▁\r\naccel_dumbbell_y\r\n0\r\n1\r\n52.08\r\n80.40\r\n-189.00\r\n-9.00\r\n41.00\r\n110.00\r\n315.00\r\n▁▇▇▅▁\r\naccel_dumbbell_z\r\n0\r\n1\r\n-38.29\r\n109.45\r\n-334.00\r\n-142.00\r\n-1.00\r\n38.00\r\n318.00\r\n▁▆▇▃▁\r\nmagnet_dumbbell_x\r\n0\r\n1\r\n-329.02\r\n339.83\r\n-643.00\r\n-535.00\r\n-480.00\r\n-307.00\r\n592.00\r\n▇▂▁▁▂\r\nmagnet_dumbbell_y\r\n0\r\n1\r\n220.31\r\n327.15\r\n-3600.00\r\n231.00\r\n310.00\r\n390.00\r\n633.00\r\n▁▁▁▁▇\r\nmagnet_dumbbell_z\r\n0\r\n1\r\n45.46\r\n139.97\r\n-262.00\r\n-46.00\r\n12.00\r\n95.00\r\n452.00\r\n▁▇▆▂▂\r\nroll_forearm\r\n0\r\n1\r\n33.46\r\n108.15\r\n-180.00\r\n-1.11\r\n21.10\r\n140.00\r\n180.00\r\n▃▂▇▂▇\r\npitch_forearm\r\n0\r\n1\r\n10.74\r\n28.10\r\n-72.50\r\n0.00\r\n9.39\r\n28.40\r\n89.80\r\n▁▁▇▃▁\r\nyaw_forearm\r\n0\r\n1\r\n19.16\r\n103.19\r\n-180.00\r\n-68.50\r\n0.00\r\n110.00\r\n180.00\r\n▅▅▇▆▇\r\ntotal_accel_forearm\r\n0\r\n1\r\n34.65\r\n10.07\r\n0.00\r\n29.00\r\n36.00\r\n41.00\r\n108.00\r\n▁▇▂▁▁\r\ngyros_forearm_x\r\n0\r\n1\r\n0.16\r\n0.65\r\n-22.00\r\n-0.22\r\n0.05\r\n0.56\r\n3.26\r\n▁▁▁▁▇\r\ngyros_forearm_y\r\n0\r\n1\r\n0.07\r\n3.29\r\n-6.65\r\n-1.48\r\n0.03\r\n1.62\r\n311.00\r\n▇▁▁▁▁\r\ngyros_forearm_z\r\n0\r\n1\r\n0.15\r\n1.94\r\n-7.94\r\n-0.18\r\n0.08\r\n0.49\r\n231.00\r\n▇▁▁▁▁\r\naccel_forearm_x\r\n0\r\n1\r\n-62.00\r\n180.67\r\n-498.00\r\n-179.00\r\n-57.00\r\n77.00\r\n477.00\r\n▂▆▇▅▁\r\naccel_forearm_y\r\n0\r\n1\r\n163.38\r\n199.43\r\n-632.00\r\n57.75\r\n200.00\r\n312.00\r\n923.00\r\n▁▂▇▅▁\r\naccel_forearm_z\r\n0\r\n1\r\n-54.90\r\n138.27\r\n-446.00\r\n-181.00\r\n-38.00\r\n26.00\r\n291.00\r\n▁▇▅▅▃\r\nmagnet_forearm_x\r\n0\r\n1\r\n-312.71\r\n346.77\r\n-1280.00\r\n-616.00\r\n-379.00\r\n-74.00\r\n672.00\r\n▁▇▇▅▁\r\nmagnet_forearm_y\r\n0\r\n1\r\n379.89\r\n508.80\r\n-896.00\r\n2.00\r\n593.00\r\n737.00\r\n1480.00\r\n▂▂▂▇▁\r\nmagnet_forearm_z\r\n0\r\n1\r\n394.70\r\n368.72\r\n-973.00\r\n192.75\r\n512.00\r\n653.00\r\n1090.00\r\n▁▁▂▇▃\r\nVariable type: POSIXct\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nmedian\r\nn_unique\r\ncvtd_timestamp\r\n0\r\n1\r\n2011-11-28 14:13:00\r\n2011-12-05 14:24:00\r\n2011-12-02 13:35:00\r\n20\r\n\r\nIt seems like the majority of the variables are numeric, but one important thing to note is that there is a high percentage of missing observations for a subset of the variables. From the completion rate, it seems that the missing values across the different attributes occur for the same observations. Since the majority of observations have missing values for these variables, it is unlikely that we could impute them. When viewing the data in spreadsheet applications these variables have a mix of being coded as NA or being simply blank, while even for observations where values are available, there are instances of a value showing as #DIV/0.\r\n\r\n\r\neda <- train %>% select(where(~ !any(is.na(.))))\r\n\r\n\r\n\r\nOne important thing in classification problems is to investigate whether there is imbalance between the different classes in our training data. For example, if a class is over-represented in the data then our classifier might tend to over-predict that class. Let’s check how many times each class appears in the data.\r\n\r\n\r\nggplot(eda, aes(classe, fill = classe)) + \r\n  geom_bar() + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the above, there does not seem to be severe imbalance among classes. We can also use a normalised version of the Shannon diversity index to understand how balanced our data set is. A value of 0 would indicate an unbalanced data set and a value of 1 would point to the data being balanced.\r\n\r\n\r\nbalance <- train %>% \r\n  group_by(classe) %>% \r\n  count() %>% \r\n  ungroup() %>% \r\n  mutate(check = - (n / sum(n) * log(n / sum(n))) / log(length(classe)) ) %>% \r\n  summarise(balance = sum(check)) %>% \r\n  pull()\r\n\r\nbalance\r\n\r\n\r\n[1] 0.9865199\r\n\r\nA value of 0.9865199 indicates we don’t have an imbalance issue.\r\nConsidering the other columns in our data, user name cannot be part of our predictors because it cannot generalize to other data if our model was used on data that is not part of this study. Looking at the timestamp, each subject was fully measured at a different point in the day, with each exercise happening back-to-back. From Velloso et al. (2013) (Velloso et al. 2013), we know that all subjects were observed in the presence of a professional trainer to observe that the exercise was done according to specification each time. I will not consider the timestamps or any features derived from them (e.g. using the time of day) as a predictor in our models.\r\n\r\n\r\neda %>% \r\n  mutate(new_window = as.numeric(factor(new_window))) %>% \r\n  select(where(is.numeric)) %>% \r\n  correlate(method = \"pearson\", quiet = TRUE) %>% \r\n  shave() %>% \r\n  rplot() %>% \r\n  ggplotly()\r\n\r\n\r\n\r\n\r\n\r\nYou can highlight a circle to see the values for which that correlation was calculated, since it’s not easy to make it out from the axes.\r\nEven excluding the columns mentioned above, there are a lot of features and presenting more information on EDA here would get too extensive. Since the purpose here is to demonstrate the modeling process with tidymodels, we will not be performing a more extensive EDA. For quick data exploration, you can use DataExplorer::create_report(eda) and explore::explore(eda) (after installing the two packages) to get a full data report on the data set from the former and a shiny app for interactive exploration from the latter.\r\nModelling\r\nIn this section we will define the recipe to pre-process the data, specify the models and combine these steps in a workflow. Then we will use cross-validation to tune the various hyper-parameters of the models.\r\nPre-processing Recipe\r\nI will use the recipes package to provide a specification of all the transformations to the data set before I fit any models, which will ensure that the same transformations are applied to the training, test and quiz data in the same way. Furthermore, it helps avoid information leakage as the transformations will be applied to all data sets using the statistics calculated for the training data. Specifically, I will remove all variables with missing values as well as other attributes discussed above, perform a transformation to try to make all predictors more symmetric (which is useful for models that benefit from predictors with distributions close to the Gaussian), normalize all variables (particularly important for the KNN and glmnet models) and then removing any predictors, if any, with very small variance. Note that we could define different pre-processing steps if required by the various models we will be tuning.\r\n\r\n\r\n# Creating a vector with the names of all variables that should be removed because they contain NAs\r\n\r\ncols_rm <- train %>% \r\n  select(where(~ any(is.na(.)))) %>% \r\n  colnames()\r\n\r\nmodel_recipe <- recipe(classe ~ ., data = train) %>% \r\n  step_rm(all_of(!!cols_rm), all_nominal(), -all_outcomes(), \r\n          cvtd_timestamp, num_window, \r\n          new_window) %>% \r\n  step_YeoJohnson(all_predictors()) %>% \r\n  step_normalize(all_predictors()) %>% \r\n  step_nzv(all_predictors())\r\n\r\n# Below rows can be used to perform the transformation on the training set. Since we will be using the workflow package, this is not required.\r\n\r\n# model_recipe_prepped <- model_recipe %>% prep()\r\n# baked_recipe <- model_recipe_prepped %>% bake(new_data = NULL)\r\n\r\n\r\n\r\nModel Specification\r\nIn this section I will use the parsnip package to create model specifications and set which model parameters will need to be tuned to ensure higher model performance. I will be trying four different models:\r\nRandom Forests (rf) - with 1000 trees and we will tune the number of predictors at each node split and the minimum number of data points in a node required for the node to be further split.\r\nK Nearest Neighbours (knn) - with a tunable number k of neighbours, kernel function with which to weight distances, and the parameter for the Minkowski distance.\r\nMultinomial Logistic Regression with Regularization (lm) - with a tunable regularization penalty.\r\nBoosted Trees (boost) - where we tune the number of trees, the learning rate, tree depth, number of predictors at each node split and the minimum number of data points in a node.\r\nI will then use the workflows package to combine the model recipe and the model specifications into different workflows. With parsnip we can create the specification in a similar manner across models and specify “computational engines” - practically R functions/packages that implement the calculations.\r\n\r\n\r\nrf_model <- rand_forest(\r\n                        mtry = tune(),\r\n                        min_n = tune(),\r\n                        trees = 1000\r\n                        ) %>% \r\n            set_mode(\"classification\") %>% \r\n            set_engine(\"ranger\")\r\n            \r\n\r\n# rf_fit <- fit(rf_model, classe ~ ., data = baked_recipe) # This call could be used to fit the model to the training data, but we will be using the workflows interface\r\n\r\nknn_model <- nearest_neighbor(\r\n                              neighbors = tune(), \r\n                              weight_func = tune(), \r\n                              dist_power = tune()\r\n                              ) %>% \r\n             set_engine(\"kknn\") %>% \r\n             set_mode(\"classification\")\r\n\r\n# knn_fit <- fit(knn_model, classe ~ ., data = baked_recipe)\r\n\r\nlasso_model <- multinom_reg(\r\n                             penalty = tune(),\r\n                             mixture = 1\r\n                             ) %>% \r\n                set_engine(\"glmnet\")\r\n\r\n# lasso_fit <- fit(lasso_model, classe ~ ., data = baked_recipe)\r\n\r\nboost_model <- boost_tree(\r\n                          trees = tune(), \r\n                          mtry = tune(),\r\n                          min_n = tune(),\r\n                          learn_rate = tune(),\r\n                          tree_depth = tune()\r\n                          ) %>% \r\n               set_engine(\"xgboost\") %>% \r\n               set_mode(\"classification\")\r\n\r\n# boost_fit <- fit(boost_model, classe ~ ., data = baked_recipe)\r\n\r\n\r\n\r\n\r\n\r\n# Combine the model and the pre-processing recipe in a workflow (per each model)\r\n\r\nrf_wf <- workflow() %>% \r\n  add_model(rf_model) %>% \r\n  add_recipe(model_recipe)\r\n\r\nknn_wf <- workflow() %>% \r\n  add_model(knn_model) %>% \r\n  add_recipe(model_recipe)\r\n\r\nlasso_wf <- workflow() %>% \r\n  add_model(lasso_model) %>% \r\n  add_recipe(model_recipe)\r\n\r\nboost_wf <- workflow() %>% \r\n  add_model(boost_model) %>% \r\n  add_recipe(model_recipe)\r\n\r\n\r\n\r\nModel Tuning\r\nLet us tune the different model parameters using 10-fold cross-validation. To create the grid with the combinations of parameters we can use a space-filling design with 30 points, based on which 30 combinations of the parameters will be picked such that they cover the most area in the design space. The dials package contains sensible default ranges for the most common hyperparameters that are tuned in models. The user can modify those if required, and for some, the default range depends on the number of features. One such example is the mtry parameter in random forests and boosted trees algorithms, whose max value is equal to the number of predictors in the processed data set. If the number of predictors is known, we can use the finalize function to assign the range for mtry. This would not be possible if our model recipe contained steps that tune the final number of predictors (e.g. pre-processing with PCA and tuning the number of components to keep).\r\n\r\n\r\n# Extract the parameters that require tuning to pass into the tuning grid\r\n\r\ntrained_data <- model_recipe %>% prep() %>% bake(new_data = NULL)\r\n\r\nrf_param <- parameters(rf_wf) %>% finalize(trained_data)\r\nknn_param <- parameters(knn_wf) \r\nlasso_param <- parameters(lasso_wf)\r\nboost_param <- parameters(boost_wf) %>% finalize(trained_data)\r\n\r\nrf_param %>% pull_dials_object(\"mtry\")\r\n\r\n\r\n# Randomly Selected Predictors (quantitative)\r\nRange: [1, 53]\r\n\r\nWhen tuning a model, it is always important to consider what we are trying to optimize the model (e.g. achieve highest possible accuracy, maximize true positives, etc). For our problem, the aim is to accurately predict the class of each observation, so at the end of the tuning process we will pick the hyperparameters that achieve highest accuracy. When tuning classification models with the tune package, by default the accuracy and area under the ROC curve are calculated for each fold. We can specify other metrics from the yardstick package to calculate while tuning by specifying the metrics parameter e.g. in tune_grid. Note that if the metrics specified perform hard class predictions (if we selected accuracy as our sole metric), then classification probabilities are not created. Since these are required for our ensemble model in a later section, we’ll also calculate the area under the curve to get the probabilities.\r\n\r\n\r\n# Split the train set into folds\r\n\r\nset.seed(9876)\r\n\r\nfolds <- vfold_cv(data = train, v = 10, strata = \"classe\")\r\n\r\n# requires the doParallel package to fit resamples in parallel\r\n\r\ncl <- makePSOCKcluster(10) # select the number of cores to parallelize the calcs across\r\nregisterDoParallel(cl)\r\n\r\nset.seed(753)\r\n\r\nrf_tune <- rf_wf %>%\r\n  tune_grid(\r\n    folds,\r\n    grid = 30,\r\n    param_info = rf_param,\r\n    control = control_grid(\r\n      verbose = TRUE,\r\n      allow_par = TRUE,\r\n      save_pred = TRUE,\r\n      save_workflow = TRUE,\r\n      parallel_over = \"resamples\"\r\n    )\r\n  )\r\n\r\n# 3423.57 sec elapsed\r\n\r\nset.seed(456)\r\n\r\nknn_tune <- knn_wf %>%\r\n  tune_grid(\r\n    folds,\r\n    grid = 30,\r\n    param_info = knn_param,\r\n    control = control_grid(\r\n      verbose = TRUE,\r\n      allow_par = TRUE,\r\n      save_pred = TRUE,\r\n      save_workflow = TRUE,\r\n      parallel_over = \"resamples\"\r\n    )\r\n  )\r\n\r\n# 8419.63 sec elapsed\r\n\r\nlasso_tune <- lasso_wf %>%\r\n  tune_grid(\r\n    folds,\r\n    grid = 30,\r\n    param_info = lasso_param,\r\n    control = control_grid(\r\n      verbose = TRUE,\r\n      allow_par = TRUE,\r\n      save_pred = TRUE,\r\n      save_workflow = TRUE,\r\n      parallel_over = \"resamples\"\r\n    )\r\n  )\r\n\r\nset.seed(1821)\r\n\r\nboost_tune <- boost_wf %>%\r\n  tune_grid(\r\n    folds,\r\n    grid = 30,\r\n    param_info = boost_param,\r\n    control = control_grid(\r\n      verbose = TRUE,\r\n      allow_par = TRUE,\r\n      save_pred = TRUE,\r\n      save_workflow = TRUE,\r\n      parallel_over = \"resamples\"\r\n    )\r\n  )\r\n\r\nstopCluster(cl)\r\n\r\n\r\n\r\nIn-sample Accuracy\r\nOne can use the collect_metrics() function to each of these to visualize the average accuracy for each combination of parameters (averaging across resamples), and see the various hyperparameters that achieve such accuracy.\r\n\r\n\r\nautoplot(rf_tune, metric = \"accuracy\")\r\n\r\n\r\n\r\n\r\nWe can see that for the random forests model a combination of around 15-20 predictors and a minimal node size in the range between 5-15 seem to be optimal.\r\n\r\n\r\nautoplot(knn_tune, metric = \"accuracy\")\r\n\r\n\r\n\r\n\r\nFor K-NN, a small number of neighbours is preferred, while Minkowski Distance of order 0.25 seems to perform best.\r\n\r\n\r\nautoplot(lasso_tune, metric = \"accuracy\")\r\n\r\n\r\n\r\n\r\nSmall penalty is preferred for the LASSO model and it seems that up to a point, similar accuracy levels are achieved.\r\n\r\n\r\nautoplot(boost_tune, metric = \"accuracy\")\r\n\r\n\r\n\r\n\r\nFor boosted trees, it seems that a higher learning rate is better. Higher tree depth (especially in the range of 9-14) seems to provide best results, while the number of trees and the minimal node size seem to have a wide range of values for which we achieve increased accuracy.\r\nLet us select the best models from each type of model and compare in-sample accuracy.\r\n\r\n\r\nbest_resamples <- \r\n  bind_rows(\r\n            show_best(rf_tune, metric = \"accuracy\", n = 1) %>% mutate(model = \"Random Forest\") %>% select(model, accuracy = mean),  \r\n            show_best(knn_tune, metric = \"accuracy\", n = 1) %>% mutate(model = \"K-NN\") %>% select(model, accuracy = mean), \r\n            show_best(lasso_tune, metric = \"accuracy\", n = 1) %>% mutate(model = \"Logistic Reg\") %>% select(model, accuracy = mean), \r\n            show_best(boost_tune, metric = \"accuracy\", n = 1) %>% mutate(model = \"Boosted Trees\") %>% select(model, accuracy = mean)\r\n  )\r\n\r\nbest_resamples %>% \r\n  arrange(desc(accuracy)) %>% \r\n  knitr::kable()\r\n\r\n\r\nmodel\r\naccuracy\r\nBoosted Trees\r\n0.9960509\r\nK-NN\r\n0.9952871\r\nRandom Forest\r\n0.9943320\r\nLogistic Reg\r\n0.7265596\r\n\r\nWe can see that the random forests, K-NN, and boosted trees models perform exceptionally on the resamples of the train data, while even the best lasso logistic regression model performs much worse than the other three. However, there is high chance that our models have overfit on the training data and actually will not perform as well when generalizing to new data. This is where out-of-sample data comes to play, as we will use the portion of the data we set aside at the beginning to calculate accuracy on new data.\r\nOut-of-sample Accuracy\r\nNow that we have a set of hyperparameters that optimize performance for each model, we can update our workflows, fit them on the entirety of the training set and perform predictions on the test set. Since the test set is part of our initial data set that we set aside, the classe variable is known and thus we can calculate accuracy. The LASSO logistic regression model probably will not be useful for prediction but for completeness I will calculate test set accuracy for all models.\r\n\r\n\r\n# Final Random Forests Workflow\r\n\r\nrf_best_accuracy <- select_best(rf_tune, metric = \"accuracy\") # retain the values of the hyperparameters that optimize accuracy\r\nrf_wf_final <- finalize_workflow(rf_wf, rf_best_accuracy) # and pass them on to the workflow\r\n\r\nset.seed(1209)\r\n\r\nrf_final_fit <- last_fit(rf_wf_final, split) # use last_fit with the split object created at the start to fit the model on the training set and predict on the test set\r\n\r\n# Final KNN\r\n\r\nknn_best_accuracy <- select_best(knn_tune, metric = \"accuracy\")\r\nknn_wf_final <- finalize_workflow(knn_wf, knn_best_accuracy)\r\n\r\nset.seed(1387)\r\n\r\nknn_final_fit <- last_fit(knn_wf_final, split) \r\n\r\n# LASSO\r\n\r\nlasso_best_accuracy <- select_best(lasso_tune, metric = \"accuracy\")\r\nlasso_wf_final <- finalize_workflow(lasso_wf, lasso_best_accuracy)\r\nlasso_final_fit <- last_fit(lasso_wf_final, split) \r\n\r\n# Final Boosted Tree\r\n\r\nboost_best_accuracy <- select_best(boost_tune, metric = \"accuracy\")\r\nboost_wf_final <- finalize_workflow(boost_wf, boost_best_accuracy)\r\n\r\nset.seed(54678)\r\n\r\nboost_final_fit <- last_fit(boost_wf_final, split)\r\n\r\n\r\n\r\n\r\n\r\nbest_oos <- bind_rows(\r\n                      rf_final_fit %>% mutate(model = \"Random Forest\"), \r\n                      knn_final_fit %>% mutate(model = \"K-NN\"), \r\n                      lasso_final_fit %>% mutate(model = \"LASSO LogReg\"), \r\n                      boost_final_fit %>% mutate(model = \"Boosted Trees\")\r\n                      ) %>% \r\n  select(model, .metrics) %>% \r\n  unnest(cols = .metrics) %>% \r\n  filter(.metric == \"accuracy\") %>% \r\n  arrange(desc(.estimate))\r\n\r\nbest_oos %>% knitr::kable()\r\n\r\n\r\nmodel\r\n.metric\r\n.estimator\r\n.estimate\r\n.config\r\nBoosted Trees\r\naccuracy\r\nmulticlass\r\n0.9982152\r\nPreprocessor1_Model1\r\nK-NN\r\naccuracy\r\nmulticlass\r\n0.9961754\r\nPreprocessor1_Model1\r\nRandom Forest\r\naccuracy\r\nmulticlass\r\n0.9943906\r\nPreprocessor1_Model1\r\nLASSO LogReg\r\naccuracy\r\nmulticlass\r\n0.7376339\r\nPreprocessor1_Model1\r\n\r\nWe can see that the boosted trees and k nearest neighbours models perform the great, with random forest trailing slightly behind. The LASSO logistic regression model has much lower performance and would not be preferred. At this point we could walk away with a model that has a 99.8% accuracy on unseen data. However, we can take it a couple of steps further to see if we can achieve even greater accuracy, as we’ll see in the next sections.\r\nEnsemble Model\r\nIn the previous section we used the tune package to try out different hyperparameter combinations over our data and estimate model accuracy using cross-validation. Let’s assume we haven’t yet tested the best model on our test data as we’re only supposed to use the test set for final selection and we shouldn’t be using the knowledge from applying to test data to improve performance. We can use the objects that were created with tune_grid to add the different model definitions to the model stack. Remember when we specified in the arguments that we save the predictions and workflows? This is because this information is required for this step, to combine the different models. Furthermore, the reason why we kept roc_auc as a metric while tuning is because it creates soft predictions, which are required in classification problems to create the stack. Since the outputs of these models will be highly correlated, the blend_predictions function performs regularization to decide which outputs will be used in the final prediction.\r\n\r\n\r\n# cl <- makePSOCKcluster(5)\r\n# registerDoParallel(cl)\r\n\r\nset.seed(5523)\r\n\r\nmodel_stack <- stacks() %>% \r\n  add_candidates(rf_tune) %>% \r\n  add_candidates(knn_tune) %>%\r\n  add_candidates(lasso_tune) %>%\r\n  add_candidates(boost_tune) %>% \r\n  blend_predictions(metric = metric_set(accuracy))\r\n\r\nmodel_stack_fit <- model_stack %>% fit_members()\r\n\r\n# stack_pred_train <- train %>% \r\n#   bind_cols(., predict(model_stack_fit, new_data = ., type = \"class\"))\r\n\r\nstack_pred_test <- test %>% \r\n  bind_cols(., predict(model_stack_fit, new_data = ., type = \"class\"))\r\n\r\n# stopCluster(cl)\r\n\r\n\r\n\r\n\r\n\r\n# stack_pred_train %>% accuracy(factor(classe), .pred_class)\r\nstack_pred_test %>% \r\n  accuracy(factor(classe), .pred_class) %>% \r\n  knitr::kable()\r\n\r\n\r\n.metric\r\n.estimator\r\n.estimate\r\naccuracy\r\nmulticlass\r\n0.9982152\r\n\r\nWe see that in the end we achieved the same accuracy as our best model, which is not unexpected considering our accuracy was almost perfect. We can also have a look at the weights of the different models used in the ensemble.\r\n\r\n\r\nautoplot(model_stack_fit, type = \"weights\") %>% \r\n  ggplotly()\r\n\r\n\r\n\r\n\r\n\r\nWe can see our ensemble assigned high weights to boost_tree models, you’ll need to zoom in to see the weights for other models 😄\r\nWhile there was not much room for improvement, as mentioned in the beginning, this was a good opportunity to play around with the new package in practice.\r\nAlthough this data set did not present much of a challenge in terms of predicting the outcome, we managed to cover many of the different steps in the modelling process using tidymodels. Further steps one could take in their analyses could potentially involve using functionality from the tidyposterior package to make statistical comparisons between the models we constructed that performed similarly. Finally, the tidymodels ecosystem of packages is constantly growing and as an example, parts of this process could be further simplified/combined using the new workflowsets package which became available on CRAN while I was working on this post.\r\n\r\n\r\n\r\nVelloso, Eduardo, Andreas Bulling, Hans Gellersen, Wallace Ugulino, and Hugo Fuks. 2013. “The 4th Augmented Human International Conference.” In. ACM Press. https://doi.org/10.1145/2459236.2459256.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-classification-modeling-workflow-using-tidymodels/classification-modeling-workflow-using-tidymodels_files/figure-html5/classe-1.png",
    "last_modified": "2021-04-12T23:23:38+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
